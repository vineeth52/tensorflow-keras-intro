{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Learning for Image Recognition\n",
    "\n",
    "In this session, we are going to build a bunch of neural nets and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "print(\"Training set has length {0} and consists of images of size {1} by {2}\".format(*train_x.shape))\n",
    "print(\"Testing set has length {0} and consists of images of size {1} by {2}\".format(*test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our data\n",
    "\n",
    "The first dataset we will work on consists of images of handwritten digits. The task at hand is to classify the image as a digit. These are a few samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "samples = train_x[np.random.choice(range(len(train_x)), size=10, replace=False), :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8,4))\n",
    "for i, sample in enumerate(samples):\n",
    "    ax = figure.add_subplot(2, 5, i+1)\n",
    "    ax.imshow(sample, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple feed forward neural net\n",
    "\n",
    "Our first approach will be to build a simple feed forward net, with no hidden layers. This is equivalent to multinomial logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have to flatten our image data into a one dimensional vector before it can be fed into a simple feed forward neural net. The input has to be normalized before feeding into the neural net. Normalization is done by dividing the each element in the vector by the max (255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "num_train_samples, width, height = train_x.shape\n",
    "num_test_samples = test_x.shape[0]\n",
    "\n",
    "flat_train_x = np.reshape(train_x, (num_train_samples, width * height))\n",
    "flat_test_x  = np.reshape(test_x, (num_test_samples, width * height))\n",
    "\n",
    "dummied_train_y = np_utils.to_categorical(train_y)\n",
    "dummied_test_y = np_utils.to_categorical(test_y)\n",
    "\n",
    "print(\"Modified training set has length {0} and consists of vectors of size {1}\".format(*flat_train_x.shape))\n",
    "print(\"Modified testing set has length {0} and consists of vectors of size {1}\".format(*flat_test_x.shape))\n",
    "print(\"Modified training labels are of size {}\".format(dummied_train_y.shape[1]))\n",
    "print(\"Modified testing labels are of size {}\".format(dummied_test_y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# normalize your input vectors\n",
    "# we do 0-1 normalization by dividing by 255\n",
    "flat_train_x = flat_train_x / 255.0\n",
    "flat_test_x = flat_test_x / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lenet_1 = Sequential()\n",
    "lenet_1.add(Dense(units=10,\n",
    "                  name=\"output\",\n",
    "                  activation=\"softmax\",\n",
    "                  input_shape=(width * height,)))\n",
    "lenet_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from lib.default_utils import default_callbacks\n",
    "\n",
    "K.set_learning_phase(True)  # important if you have modules like dropout in your model\n",
    "\n",
    "lenet_1.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "logpath, dfcb = default_callbacks(lenet_1, prefix='lenet-1', batch_size=32)\n",
    "\n",
    "lenet_1.fit(x=flat_train_x, \n",
    "          y=dummied_train_y, \n",
    "          batch_size=32, \n",
    "          epochs=10, \n",
    "          verbose=True, \n",
    "          callbacks=dfcb, \n",
    "          validation_split=0.2,\n",
    "          shuffle=True)\n",
    "\n",
    "# save final weights after completion of training\n",
    "lenet_1.save_weights(os.path.join(logpath, \"model_weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate on test_dataset\n",
    "_, test_accuracy = lenet_1.evaluate(flat_test_x, dummied_test_y)\n",
    "print(\"Model accuracy on test dataset is {:.3f}\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural net with one hidden layer\n",
    "\n",
    "We add more complexity by adding one hidden layer into our network. We will compare the performance of this network with the previous model using tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lenet_2 = Sequential()\n",
    "lenet_2.add(Dense(units=16,\n",
    "                  name=\"hidden_1\",\n",
    "                  activation=\"relu\",\n",
    "                  input_shape=(width * height,)))\n",
    "\n",
    "lenet_2.add(Dense(units=10,\n",
    "                  name=\"output\",\n",
    "                  activation=\"softmax\"))\n",
    "lenet_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## build lenet-2\n",
    "lenet_2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "logpath, dfcb = default_callbacks(lenet_2, prefix='lenet-2', batch_size=32)\n",
    "\n",
    "lenet_2.fit(x=flat_train_x, \n",
    "          y=dummied_train_y, \n",
    "          batch_size=32, \n",
    "          epochs=20, \n",
    "          verbose=True, \n",
    "          callbacks=dfcb, \n",
    "          validation_split=0.2,\n",
    "          shuffle=True)\n",
    "\n",
    "# save final weights after completion of training\n",
    "lenet_2.save_weights(os.path.join(logpath, \"model_weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate on test_dataset\n",
    "_, test_accuracy = lenet_2.evaluate(flat_test_x, dummied_test_y)\n",
    "print(\"Model accuracy on test dataset is {:.3f}\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Simple convolutional net\n",
    "\n",
    "Regular neural nets don't scale well for images. Natural images have a structure which we can hopefully exploit. Convolutional neural nets exploit this structure to perform image related classification tasks.\n",
    "\n",
    "Convolution is a linear operation which is very commonly used in image processing. For example, blurring an image is done by convolving the image with a gaussian filter. Traditionally, edge detectors (formulated as convolution) had been used in the past as feature extractors for image related tasks. CNNs take inspiration from this idea. CNNs learn custom filters based on the task and need no hand-built detectors.\n",
    "\n",
    "Convolution is implemented using sliding window filters for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# lenet 3\n",
    "# 2 convolutional layers - 3x3 and 5x5 patches\n",
    "from keras.layers import Conv2D, Flatten\n",
    "\n",
    "lenet_3 = Sequential()\n",
    "lenet_3.add(Conv2D(filters=32,\n",
    "                  name=\"conv_1\",\n",
    "                  kernel_size=(3,3),\n",
    "                  activation=\"relu\",\n",
    "                  padding='valid',\n",
    "                  input_shape=(width, height, 1)))\n",
    "\n",
    "lenet_3.add(Conv2D(filters=12,\n",
    "                  kernel_size=(5,5),\n",
    "                  padding='valid',\n",
    "                  name=\"conv_2\",\n",
    "                  activation=\"relu\"))\n",
    "\n",
    "lenet_3.add(Flatten())\n",
    "lenet_3.add(Dense(units=10,\n",
    "                  name='output',\n",
    "                  activation='softmax'))\n",
    "lenet_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "train_x = np.expand_dims(train_x, axis=-1)\n",
    "test_x = np.expand_dims(test_x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## build lenet-3\n",
    "lenet_3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "logpath, dfcb = default_callbacks(lenet_3, prefix='conv-lenet-3', batch_size=32)\n",
    "    \n",
    "lenet_3.fit(x=train_x, \n",
    "            y=dummied_train_y, \n",
    "            batch_size=32, \n",
    "            epochs=10, \n",
    "            verbose=True, \n",
    "            callbacks=dfcb, \n",
    "            validation_split=0.2,\n",
    "            shuffle=True)\n",
    "\n",
    "# save final weights after completion of training\n",
    "lenet_3.save_weights(os.path.join(logpath, \"model_weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate on test_dataset\n",
    "_, test_accuracy = lenet_3.evaluate(test_x, dummied_test_y)\n",
    "print(\"Model accuracy on test dataset is {:.3f}\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Miscellaneous stuff\n",
    "\n",
    " - Overfitting\n",
    " \n",
    " Neural networks are notoriously easy to overfit, make sure your data set is big enough for the model that you are building. Always use a large validation set. Cross-validation can be time consuming.\n",
    " \n",
    " - Learning rate decay\n",
    " \n",
    " Always normalize your data before you feed it into the model. Gradient descent can be difficult to converge/tune without normalization\n",
    "\n",
    " - optimizers\n",
    " \n",
    " There is a large variety of optimizers out there - sgd with momentum, adam, rmsprop. We recommend sticking to sgd if you want good generalization. [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://arxiv.org/abs/1705.08292)\n",
    " \n",
    " - pooling in convolutional nets\n",
    " \n",
    " Feature pooling is a way to reduce feature size as you go deeper in the neural net.\n",
    " \n",
    " - class imbalance\n",
    " \n",
    " Make sure your classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Exercise] Add dropout to the model\n",
    "\n",
    "Dropout is a way to prevent overfitting. add more explanation, insert link to paper.\n",
    "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "lenet_4 = Sequential()\n",
    "lenet_4.add(Conv2D(filters=32,\n",
    "                  name=\"conv_1\",\n",
    "                  kernel_size=(3,3),\n",
    "                  activation=\"relu\",\n",
    "                  padding='valid',\n",
    "                  input_shape=(width, height, 1)))\n",
    "\n",
    "lenet_4.add(Dropout(0.2))\n",
    "\n",
    "lenet_4.add(Conv2D(filters=12,\n",
    "                  kernel_size=(5,5),\n",
    "                  padding='valid',\n",
    "                  name=\"conv_2\",\n",
    "                  activation=\"relu\"))\n",
    "\n",
    "lenet_4.add(Dropout(0.2))\n",
    "\n",
    "lenet_4.add(Flatten())\n",
    "lenet_4.add(Dense(units=10,\n",
    "                  name='output',\n",
    "                  activation='softmax'))\n",
    "lenet_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "lenet_4.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "logpath, dfcb = default_callbacks(lenet_4, prefix='conv-lenet-4-dropout', batch_size=32)\n",
    "    \n",
    "# start training, use tensorboard to show overfitting with more iterations\n",
    "lenet_4.fit(x=train_x, \n",
    "            y=dummied_train_y, \n",
    "            batch_size=32, \n",
    "            epochs=10, \n",
    "            verbose=True, \n",
    "            callbacks=dfcb, \n",
    "            validation_split=0.2,\n",
    "            shuffle=True)\n",
    "\n",
    "# save final weights after completion of training\n",
    "lenet_4.save_weights(os.path.join(logpath, \"model_weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate on test_dataset\n",
    "K.set_learning_phase(False)\n",
    "_, test_accuracy = lenet_4.evaluate(test_x, dummied_test_y)\n",
    "print(\"Model accuracy on test dataset is {:.3f}\".format(test_accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
